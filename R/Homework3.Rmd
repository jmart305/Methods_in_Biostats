---
title: "Homework 3"
author: "Julia Martin"
date: "2024-09-30"
output:
  pdf_document: default
  html_document: default
---

### Part 1b

```{r}
library(ggplot2)
likelihood <- function(p) {
  p^7*(1-p)^3
}
phat <- 0.7
max_likelihood <- likelihood(phat)
# renormalize:
renormalizing_constant <- 1 / likelihood(phat)

x <- seq(0,1,0.01)
y <- renormalizing_constant*likelihood(x)
lplot <- data.frame(cbind(x,y))
ggplot(lplot, aes(x, y)) +
  geom_line() + xlab("Probability") + ylab("Likelihood") + ggtitle("Likelihood Function for Part a")
```

This likelihood suggests that the coin is not fair, since the likelihood function plot is right skewed, meaning there is a higher likelihood of the probability of flipping heads to be greater than 0.5, meaning that the coin is not fair. We can also calculate the likelihood ratio of the coin being fair (0 = 0.5) vs. p = 0.7.

```{r}
likelihood(0.5) / likelihood(0.7)
```

Since this ratio is 0.439, which is less than 1, there is more evidence supporting that the coin has a probability of 0.7 of getting heads compared to 0.5, which suggests that it is unfair.

\newpage

### Part 1e

```{r}
y <- 10
likelihood2 <- function(p) {
  y <- 10
  choose(y-1,2) * ((1-p)^3) * (p^(y-3))
}
phat <- 7/10
max_likelihood <- likelihood2(phat)
# renormalize:
renormalizing_constant <- 1 / likelihood2(phat)

x <- seq(0,1,0.01)
y <- likelihood2(x)
lplot <- data.frame(cbind(x,y))
ggplot(lplot, aes(x, y)) +
  geom_line() + xlab("Probability") + ylab("Likelihood") + ggtitle("Likelihood Function for Part e")
```

This has the same likelihood function as in part b. Similar to part b, this likelihood plot suggests that the coin is not fair because it is skewed.

### Problem 1f

```{r}
p_value <- 1 - choose(8,2)*0.5^9 - choose(7,2)*0.5^8 - choose(6,2)*0.5^7 - choose(5,2)*0.5^6 - choose(4,2)*0.5^5 - choose(3,2)*0.5^4 - choose(2,2)*0.5^3
p_value
```

This p-value is lower than the one obtained in part c, but using an $\alpha$ of 0.05, this p-value is still greater than the significance level, so this suggests that the coin is fair.

### Problem 2a

```{r}
pnorm(90, 80, 12)
```

The probability that a randomly selected person from this population has DBP less than 90 is 0.52768.

### Problem 2b

```{r}
qnorm(0.9, 80, 12)
qnorm(0.95, 80, 12)
qnorm(0.975, 80, 12)
```

90th percentile: 95.37862 95th percentile: 99.73824 97.5th percentile: 103.5196

### Problem 2c

```{r}
# SBP:

1 - pnorm(131,120,11)
1 - pnorm(142,120,11)
1 - pnorm(153,120,11)

# DBP:

1 - pnorm(92, 80, 12)
1 - pnorm(104, 80, 12)
1 - pnorm(116, 80, 12)
```
I am interpreting the question as having at least 1, 2, or 3 standard deviations above the mean.  
The probability of a random person having a SBP 1 standard deviation above the mean is 0.1586553. 
The probability of a random person having a SBP 2 standard deviations above the mean is 0.02275013.  
The probability of a random person having a SBP 3 standard deviations above the mean is 0.001349898.  
The probability of a random person having a DBP 1 standard deviation above the mean is 0.1586553.  
The probability of a random person having a DBP 2 standard deviations above the mean is 0.02275013.   
The probability of a random person having a DBP 3 standard deviations above the mean is 0.001349898.  

The probabilities are the same for SBP and DBP.

### Problem 2d

```{r}
# can be modeled using a binomial distribution
# probability of success:
p <- 1 - pnorm(140,120,11)
dbinom(5, 10, p)
```

The probability that 5 of 10 of the sample have an SBP larger than 140 is 1.036001e-05.

### Problem 2e

```{r}
dbinom(500, 1000, p)
```

The probability that 5 of 10 of the sample have an SBP larger than 140 is 0.

### Problem 2f

```{r}
(1 - pnorm(90, 80, 12)) * (1 - pnorm(140, 120, 11))
```

The probability that someone has an SBP greater than 140 and DBP greater than 90 is 0.006984006 if they are independent. Independence is probably not a good assumption because there is a very small chance of both of these events occurring under independence, when in reality there are probably many people with these metrics and they are most likely correlated.

### Problem 2g

```{r}
# I am calculating the probability that the average DBP is smaller than 81.3 
# for sample size of 200
# I can use the central limit theorem because the probability of the normalized 
# sample mean converges to the standard normal distribution  
Z <- (81.3 - 80) / (12/sqrt(200))
pnorm(Z, 0 , 1)
```

The probability that this average is smaller than 81.3 is 0.9372468.

### Problem 3a

```{r}
set.seed(7)
exp_sample <- rexp(1000,1)
mean(exp_sample)
var(exp_sample)
```

The sample mean should estimate $\mu$, the theoretical mean (1), and the sample variance should estimate $\sigma^2$, the theoretical variance (1). This is because of the law of large numbers, which states that the sample mean converges to $\mu$ and functions of convergent random sequences converge to the function evaluated at that limit, and since the sample variance is a function of the random sequence, it converges to the function evaluated at the limit, which returns $\sigma^2$.

### Problem 3b

```{r}
seq_sample <- data.frame(cbind(cumsum(exp_sample) / 1:1000, 1:1000))
colnames(seq_sample) <- c("Sample_Mean", "n")
ggplot(seq_sample, aes(n, Sample_Mean)) + 
  geom_line(cex=0.7, color = "purple") + ggtitle("Sequential Sample Means") + geom_line(y = 1, color = "red", linewidth = 0.3)
```

This plot shows that as the number of samples increases, the sample mean converges to 1, which is the true mean from which these samples were drawn. This supports the law of large numbers.

### Problem 3c

```{r}
ggplot(data.frame(exp_sample), aes(exp_sample)) + 
  geom_histogram(bins = 30, color = "black", fill = "lightblue") + xlab("Exponential Sample Value") + ylab("Frequency") + ggtitle("Histogram of Exponential Sample")
```

Yes, this histogram looks like an unsmoothed version of an exponential density.

### Problem 3d

```{r}
sample_means <- rep(NA, 1000)
for (i in 1:1000) {
  sample_obs <- rexp(100,1) 
  sample_means[i] <- mean(sample_obs)
}
mean(sample_means)
var(sample_means)
```

The average of the sample means should be 1, which is the theoretical mean from which the data was drawn, and the variance of the sample means should be 0.01, which is $\sigma / n$, or $1 / 100 = 0.01$

This is because of the Central Limit Theorem, which states that the distribution of a sample mean, given enough observations, can be approximated by a normal distribution with mean $\mu$ and variance $\sigma / n$, and can be scaled using those values to become a standard normal distribution. 

### Problem 3e

```{r}
Z <- (sample_means - 1) / (1/sqrt(100))
ggplot(data.frame(Z), aes(Z)) +
  geom_histogram(color = "black", fill = "lightblue", bins = 35) + ggtitle("Histogram of Sample Means") + xlab("Scaled Sample Means")
```

This plot looks like a standard normal distribution, which makes sense because sample means calculated from drawing from a given distribution, then standardized, follow the standard normal distribution. This follows from the Central Limit Theorem. 

### Problem 3f

```{r}
sample_vars <- rep(NA, 1000)
for (i in 1:1000) {
  sample_obs <- rexp(100,1) 
  sample_vars[i] <- var(sample_obs)
}
mean(sample_vars)
```

This shows that the sample variance converges to the theoretical/population variance (1), which follows from the law of large numbers. This shows that the sample variance is a consistent estimator. We also know that the expectation of the sample variance is the theoretical/population variance, so it is an unbiased estimator too. 




